---
title: QuPath for Python Programmers üêç
author: Alan O'Callaghan, L√©o Leplat, Peter Bankhead, Fiona Inglis, Laura Nicol√°s S√°enz
format: revealjs
echo: true
jupyter: python3
---


## Introduction/motivation

QuPath is user-friendly for analysing large images interactively

Python is great for developing and implementing image analysis algorithms with code

They don't *always* cooperate nicely... but this is changing.


---

## Paquo

Paquo is an existing library for QuPath-python
interactions.

*but* it aims to provide 



---

## Setting up a gateway

The main method of interacting with QuPath
is through a *gateway*, which operates using
a websocket connection:

```{python}
from qubalab.qupath import qupath_gateway

token = None
port = 25333
gateway = qupath_gateway.create_gateway(auth_token=token, port=port)

gateway
```

---

## Basic gateway operations

A gateway provides us with an *entry point*,
which allows us to call Java methods from python:


```{python}
print(f"Extension version: {gateway.entry_point.getExtensionVersion()}")
print(f"Current image name: {gateway.entry_point.getCurrentImageName()}")
```

---

## Mixing üêç and ‚òï

Since the entry point exposes the QuPath scripting interface, you can do lots of basic scripting operations in python by calling object methods.

```{python}
cores = gateway.entry_point.getTMACoreList()
tumor_cores = [core for core in cores if core.getClassification() == "Tumor"]
[(core.getName(), core.getChildObjects().size()) for core in tumor_cores]
```

---

## Downsides

However, it's not ideal to write Java code in python, and we wouldn't necessarily encourage it.

If you just want to script QuPath, groovy will
remain the best bet.


## Images

```{python, echo=FALSE}
cache_folder = "./images"

# Define a utility function to find or download an image

from pathlib import Path
import urllib.request

if cache_folder != "":
    Path(cache_folder).mkdir(parents=True, exist_ok=True)

def get_image(image_name, image_url):
    if cache_folder == "":
        filename = None
    else:
        filename = Path(cache_folder) / image_name
    
    if filename is None or not(filename.exists()):
        print(f"Downloading {image_name}...")
        path, _ = urllib.request.urlretrieve(image_url, filename=filename)
        print(f'{image_name} saved to {path}')
    else:
        path = filename
        print(f'{image_name} found in {path}')

    return path
```

```{python, echo=FALSE}
# Download or get image
cmu_path = get_image("CMU-1.svs", "https://openslide.cs.cmu.edu/download/openslide-testdata/Aperio/CMU-1.svs")
```

```{python}
from qubalab.images.openslide_server import OpenSlideServer


# Create the ImageServer from the image path. This will read the image metadata but not the pixel values yet.
# This function has optional parameters you can find in the documentation
openslide_server = OpenSlideServer(cmu_path)
```


```{python, echo=FALSE}
# Download or get image
fluoro_path = get_image("patient_test_2.ome.tiff", "https://ftp.ebi.ac.uk/biostudies/fire/S-BIAD/463/S-BIAD463/Files/my_submission/Validation_raw/DCIS/Patient_test_2.ome.tiff")
```


```{python}
from qubalab.images.aicsimageio_server import AICSImageIoServer

# Create the ImageServer from the image path. This will read the image metadata but not the pixel values yet.
# This function has optional parameters you can find in the documentation
aicsimageio_server = AICSImageIoServer(fluoro_path)
```

```{python}
from qubalab.images.icc_profile_server import IccProfileServer

# Create the IccProfileServer from the existing OpenslideServer. The two servers will have the same metadata, but pixel values might differ a bit
icc_profile_server = IccProfileServer(openslide_server)
```

```{python}
server = openslide_server
```


```{python}
# Access image metadata

metadata = server.metadata

print(f'Image path: {metadata.path}')
print(f'Image name: {metadata.name}')
print()

print('Levels:')
for level, shape in enumerate(metadata.shapes):
    print(f'Shape of level {level}: {shape}')
print()

print('Pixel calibration:')
print(f'Pixel length on x-axis: {metadata.pixel_calibration.length_x}')
print(f'Pixel length on y-axis: {metadata.pixel_calibration.length_y}')
print()

print(f'Pixel type: {metadata.dtype}')
print()

print(f'Downsamples: {metadata.downsamples}')
print()

print('Channels:')
for channel in metadata.channels:
    print(channel)
```


```{python}
# Read and show lowest resolution image with read_region

highest_downsample = server.metadata.downsamples[-1]
lowest_resolution = server.read_region(highest_downsample)

print(f'Image shape: {lowest_resolution.shape}')

# This calls a utility function from qubalab to plot the image
# If the image is RGB, the entire image is plotted
# Otherwise, only the first channel is plotted
from qubalab.display.plot import plotImage
import matplotlib.pyplot as plt
_, ax = plt.subplots()
plotImage(ax, lowest_resolution)
```

```{python}
downsample = 1
x = 13000
y = 15000
width = 2000
height = 1000
tile = server.read_region(downsample, x=x, y=y, width=width, height=height)

print(f'Tile shape: {tile.shape}')

_, ax = plt.subplots()
plotImage(ax, tile)
```


```{python}
last_level = server.metadata.n_resolutions - 1
lowest_resolution = server.level_to_dask(last_level)

# Pixel values are not read yet, but you can get the shape of the image
print(f'Image shape: {lowest_resolution.shape}')

# Compute array. This will read the pixel values
lowest_resolution = lowest_resolution.compute()

_, ax = plt.subplots()
plotImage(ax, lowest_resolution)
```

```{python}
first_level = 0
highest_resolution = server.level_to_dask(first_level)

print(f'Full resolution image shape: {highest_resolution.shape}')

# Only read a 2000x1000 pixels tile whose top left pixel is located at x=13000 and y=15000 on the full resolution image
x = 13000
y = 15000
width = 2000
height = 1000
tile = highest_resolution[:, y:y+height, x:x+width]

print(f'Tile shape: {tile.shape}')

# Compute array. This will only read the pixel values of the tile, not the entire image
tile = tile.compute()

_, ax = plt.subplots()
plotImage(ax, tile)
```

---

## to_dask

It's also possible to read a dask array at arbitrary downsample using `to_dask`, but this is currently quite expensive and not recommended.

---

## Objects


```{python}
from qubalab.objects.object_type import ObjectType

object_type = ObjectType.ANNOTATION    # could be DETECTION, TILE, CELL, TMA_CORE
```

```{python}
annotations = qupath_gateway.get_objects(object_type = object_type)

for annotation in annotations:
    print(annotation)
```


---

## Changing QuPath objects

Communicating with QuPath can be two-way:

```{python}
annotation = annotations[0]

annotation.setName("Hello from Python")
```

```{python, echo=FALSE}
qupath_gateway.refresh_qupath()

plt.imshow(qupath_gateway.create_snapshot())
plt.axis(False)
plt.show()
```


```{python}
annotations = qupath_gateway.get_objects(object_type = object_type, converter='geojson')

print(type(annotations[0]))
```

```{python}
from shapely.geometry import shape

shape(annotations[0].geometry)
```

```{python}
import random

for annotation in annotations:
    if not annotation.classification:
        annotation.color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))

qupath_gateway.delete_objects(object_type = object_type)
qupath_gateway.add_objects(annotations)
```

```{python}
from qubalab.images.qupath_server import QuPathServer
from qubalab.images.labeled_server import LabeledImageServer

# Create a QuPathServer that represents the image currently opened in QuPath
qupath_server = QuPathServer(gateway)

# Set a downsample. The labeled image will be 20 times smaller than the image currently opened in QuPath
downsample = 20

# Create the LabeledImageServer. This doesn't create labeled image yet
labeled_server = LabeledImageServer(qupath_server.metadata, annotations, downsample=downsample)

# Request the pixel values of the entire labeled image. Pixel values will be created as they are requested 
label_image = labeled_server.read_region()

# label_image is an image of shape (c, y, x), not easily plottable
# We use a utility function from qubalab to plot the image
from qubalab.display.plot import plotImage
import matplotlib.pyplot as plt
_, ax = plt.subplots()
plotImage(ax, label_image)
```


```{python}
# Create another LabeledImageServer, which will represent a stack of masks here
mask_server = LabeledImageServer(qupath_server.metadata, annotations, downsample=downsample, multichannel=True)

# Compute and return the masks
masks = mask_server.read_region()

# masks contains (n+1) channels, where n is the number of annotations
# The i channel corresponds to the mask representing the i annotation
# Let's plot the first mask corresponding to the first annotation
_, ax = plt.subplots()
plotImage(ax, masks, channel=1)
```


```{python}
import numpy as np
import matplotlib.pyplot as plt
from skimage.filters import gaussian, threshold_otsu, threshold_triangle
from skimage.color import rgb2gray
from qubalab.objects.image_feature import ImageFeature


# Set a downsample. The labeled image will be 20 times smaller than the image currently opened in QuPath
downsample = 20

# Set different threshold methods to apply to the image
threshold_methods = {
    'Otsu' : threshold_otsu,
    'Triangle' : threshold_triangle
}

# Read image opened in QuPath
image = qupath_server.read_region(downsample=downsample)

# Convert the image to greyscale
if qupath_server.metadata.is_rgb:
    # If the image is RGB, we convert it to grayscale
    # read_region() returns an image with the (c, y, x) shape.
    # To use rgb2gray, we need to move the channel axis so that
    # the shape becomes (y, x, c)
    image = np.moveaxis(image, 0, -1)
    image = rgb2gray(image)
else:
    # Else, we only consider the first channel of the image
    image = image[0, ...]

# Apply a gaussian filter
image = gaussian(image, 2.0)

# Iterate over threshold methods
for i, (name, method) in enumerate(threshold_methods.items()):
    # Apply threshold to image
    threshold = method(image)

    # Create mask from threshold
    mask = image < threshold

    # Create annotations from mask
    annotations = ImageFeature.create_from_label_image(
        mask,   
        scale=downsample,   # mask is 20 times smaller than the QuPath image, so we scale
                            # the annotations to fit the QuPath image
        classification_names=name,  # set a single classification to the detected annotations
    )

    # Add annotations to QuPath
    qupath_gateway.add_objects(annotations)

    # Plot mask
    plt.subplot(1, len(threshold_methods), i+1)
    plt.imshow(mask)
    plt.title(f'{name} (threshold={threshold:.2f})')
    plt.axis(False)
plt.show()
```


```{python}
# qupath_gateway.delete_objects()
```

---

## Displaying objects

We don't *need* QuPath to visualize GeoJSON features.

QuBaLab also includes functionality for generating matplotlib plots that *look* a lot like QuPath plots... but that don't use QuPath.

**Before running the next notebook cell**, you should draw a few small annotations in QuPath and detect cells within them.

The plotting code will show the annotations and cells - randomly recoloring the cells, to demonstrate that they are distinct from QuPath's rendering.

```{python}
# from qubalab.display.plot import plotImageFeatures

# # Get QuPath annotations and detections
# annotations = qupath_gateway.get_objects(object_type = ObjectType.ANNOTATION, converter='geojson')
# detections = qupath_gateway.get_objects(object_type = ObjectType.DETECTION, converter='geojson')

# if len(detections) == 0:
#     print("No detections found. Please run cell detection from QuPath before running this cell.")

# # Set a random color for each detection
# for detection in detections:
#     detection.color = [random.randint(0, 255) for _ in range(3)]

# # Plot every annotations and their detections
# fig = plt.figure(figsize=(10, 8))
# for i, annotation in enumerate(annotations):
#     ax = fig.add_subplot(len(annotations), 1, i+1)

#     # Invert y axis. This is needed because in QuPath, the y-axis is going down
#     ax.invert_yaxis()

#     # Set title of graph from annotation name. You won't see it if the annotation doesn't have a name
#     ax.set_title(annotation.name)

#     # Plot annotation
#     plotImageFeatures(ax, [annotation], region=annotation.geometry, fill_opacity=0.1)

#     # Plot detections that are located below the annotation
#     plotImageFeatures(ax, detections, region=annotation.geometry, fill_opacity=0.25)
```

---

```{python}
# from shapely.geometry import shape

# # An offset in pixels to also see the image around the annotations
# offset = 100

# # Plot every annotations, their detections, and the image below it
# fig = plt.figure(figsize=(10, 8))
# for i, annotation in enumerate(annotations):
#     ax = fig.add_subplot(len(annotations), 1, i+1)

#     # Set title of graph from annotation name. You won't see it if the annotation doesn't have a name
#     ax.set_title(annotation.name)

#     # Compute bounds of the image. We add a small offset to see the image around the annotation
#     bounds = shape(annotation.geometry).bounds
#     min_x = max(0, bounds[0] - offset)
#     min_y = max(0, bounds[1] - offset)
#     max_x = min(qupath_server.metadata.width, bounds[2] + offset)
#     max_y = min(qupath_server.metadata.height, bounds[3] + offset)

#     # Get pixel values of image
#     image = qupath_server.read_region(x=min_x, y=min_y, width=max_x-min_x, height=max_y-min_y)

#     # Plot annotation
#     plotImageFeatures(ax, [annotation], region=annotation.geometry, fill_opacity=0.1)

#     # Plot detections that are located below the annotation
#     plotImageFeatures(ax, detections, region=annotation.geometry, fill_opacity=0.25)

#     # Plot image
#     plotImage(ax, image, offset=[min_x, min_y, max_x, max_y])
```


---

## Other workflow ideas

Object classifiers using scikit-learn

Object clustering using graph clustering

Assessing classifier feature importance using UMAP


---

## Summary
